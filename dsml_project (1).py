# -*- coding: utf-8 -*-
"""DSML_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A9lixOuiP85Mm23sHEHs1kioY1bd4JWq

**Title of ML project: INCOME LEVEL CLASSIFICATION PREDICTIONS**

**Name:** JUBIN K. BABU.

**Organization:** Entri Elevate

**Date:** [Evaluation Date]

#1.Overview of Problem Statement

The ML project aims to predict whether an individual’s income is above or below $50K using the Census Income Dataset from UCI. The dataset includes demographic and work-related features like age, education, occupation, work hours, and capital gain/loss. The workflow involves data collection,taking basic details of data,data preprocessing, exploratory data analysis (EDA), model training using classification algorithms (Logistic Regression, Decision Trees, Random Forest,GradientBoosting etc...) and performance evaluation. The best-performing model will be optimized and deployed for practical use. This project provides insights into income disparity factors and can be useful in HR analytics, financial planning, and policy-making

# 2.Objective

Develop a machine learning model to predict whether an individual earns more than $50K per year based on demographic and employment-related attributes from the UCI Census Income dataset. This model can be useful for socioeconomic analysis, government policy planning, and targeted financial services.

#3.Data Description

[UCI Census Income dataset (also known as "Adult" dataset)]

Features: [age,workclass,fnlwgt,education,education-num,marital-status, occupation,relationship,race,sex,capital-gain,capital-loss,hours-per-week, native-country,income]

**Importing Necesscary Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import joblib
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score, roc_curve, auc,confusion_matrix
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import SelectKBest,f_classif
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report

"""#4.Data Collection"""

#  Defining column names based on dataset
columns = ["age", "workclass", "fnlwgt", "education", "education-num",
           "marital-status", "occupation", "relationship", "race", "sex",
           "capital-gain", "capital-loss", "hours-per-week", "native-country", "income"]

# Load the .data and .test files into DataFrames
df_train = pd.read_csv("/content/adult.data", names=columns, skipinitialspace=True)
df_test = pd.read_csv("/content/adult.test", names=columns, skipinitialspace=True, skiprows=1)

# Display first 6 rows
df_train.head()
df_test.head()

"""**CONVERTING TO DATAFRAME**"""

# Concatenate the training and testing datasets
df = pd.concat([df_train, df_test], axis=0)

# Reset the index of the combined DataFrame
df = df.reset_index(drop=True)
df.head()

df.sample(5) #To take the random rows in the dataframe as sample

"""Above is the sample rows printed in which it has "?" as the element."""

df.replace("?",np.nan,inplace=True)

"""There are so many columns which has "?" as an element,So I replaced it with nan value and then handled it with necesscary imputation"""

df.sample(10)

"""Above is the sample rows printed in which it has no "?".

# Basic info about the dataset
"""

df.shape

""" Above tuple represnts the number of instances and the features."""

df.info()

df.describe()

"""The dataset represents a working-age population (mostly 28–48 years old).

Most people have high school-level education.

Only a few have significant capital gains/losses.

The standard workweek is around 40 hours, but some work extremely high or low hours.

# 5.Data Preprocessing
"""

df.isnull().sum()

"""There are so many null values in certian columns like "workclass","occupation,"native-country",So we need to remove these."""

df.isnull().sum()/100 #to get the percentage of null values in each column.

"""The above metrices represents the percentage of null values an found that workclass has 27.99%,occupation has 28.09% and native country has 8.57% null values.

**Separating numerical and categorical columns**
"""

num_cols=df.select_dtypes(include="number").columns
num_cols

cat_cols=df.select_dtypes(include="object").columns
cat_cols

"""**Imputation of null values**"""

imputer = SimpleImputer(strategy='most_frequent')
df[cat_cols] = imputer.fit_transform(df[cat_cols])

"""The above uses mode impuatation strategy for imputing null values in the  categorical columns."""

df.isnull().sum()

df.duplicated().sum()

"""We found that there are 29 duplicate rows,So we need to handle it by drop_duplicates method below."""

df.drop_duplicates(inplace=True)
df.head()

df.shape

df["income"].unique()

"""We found that there are four different classes but it originally representing only two classes, this happened due to typo, So we replaced it in below cell with appropriate values."""

# Replace income values
df["income"] = df["income"].replace({"<=50K.": "<50K", "<=50K": "<50K",">50K.": ">50K"})
df["income"].value_counts()
df

"""Again rechecking the value counts"""

df["income"].value_counts()

df.describe()

"""**Checking the skewness**"""

df.select_dtypes(include="number").skew()

"""Here we found that the most outliers are  present in the columns "capital-loss" and capital-gain", and other columns like ("age","hours-per-week","education-num") have only minimal outliers which can be neglected.So we need to remove outliers of ("capital-gain","capital-loss","fnlwgt") columns."""

sns.boxplot(df)

"""We plotted this boxplot to get a rough idea about the outliers in each columns so we can visualize the amount.And this shows that "fnlwgt" has so many outliers.So We remove this outlier using Capping by IQR method.

**Capping Outliers using IQR method**
"""

#Creating a user-defined function for capping the outliers
import numpy as np
def cap_outliers(df, columns):
    df_capped = df.copy()
    for col in columns:
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        df_capped[col] = np.where(df_capped[col] < lower_bound, lower_bound, df_capped[col])
        df_capped[col] = np.where(df_capped[col] > upper_bound, upper_bound, df_capped[col])
    return df_capped

"""So above we created a user defined function for Capping method usinf IQR strategy.And then applied to  the numerical columns of this dataframe."""

df_capped = cap_outliers(df, num_cols)
df_capped

df_capped.select_dtypes(include="number").skew()

"""Above we again rechecked fo the outliers and found that it is mostly removed those."""

# Apply Log Transformation
df_capped["capital-gain"] = np.log1p(df_capped["capital-gain"])
df_capped["capital-loss"] = np.log1p(df_capped["capital-loss"])

sns.boxplot(df_capped)

"""Above again we plotted the boxplot and found that all outliers are removed.

# 6.Exploratory Data Analysis(EDA)

**Histogram to check the skewness**
"""

df_capped.hist(figsize=(15,10),bins=30)

"""The age and education levels are well-distributed.
The capital-gain and capital-loss features are highly imbalanced and may need transformation.
Hours worked per week shows a strong concentration at 40 hours, indicating many individuals work full-time

**KDE Plots**
"""

for col in num_cols:
  plt.figure(figsize=(6,5))
  sns.histplot(df_capped[col],kde=True,bins=30)
  plt.show()

"""**Pie Chart**"""

target_count = df_capped['income'].value_counts()
plt.figure(figsize=(5,5))
plt.pie(target_count, labels=target_count.index, autopct='%1.1f%%', startangle=90)

"""The above pie chart representing the classes of "income" columns and it shows that 76.1% of data represents peoples with income less than 50K dollars and remaining 23.9% represents the peoples with income greater than 50K dollars

**Count plot**
"""

plt.figure(figsize=(19,6))
sns.countplot(data=df_capped,x="education",palette="viridis")

"""The above graph represent the count of unique class in the "education" column.And it shows that most of peoples educational qualification are HS-grad"""

plt.figure(figsize=(11,6))
sns.countplot(data=df_capped,x="marital-status",palette="viridis")

"""The above graph represent the count of unique class in the "marital-status" column.It shows that most of people belongs to "Married-civ-spouse" class."""

plt.figure(figsize=(6,4))
sns.countplot(data=df_capped,x="income",palette="viridis")

"""The above graph represent the count of unique class in the "income" column.This graph suggest that the income column is highly imbalanced and it tells that people with salary less than 50 dollars are in high number.So inorder to do the classification tasks we need to balance the column and we did it using "SMOTE" technique.

**Feature Encoding**
"""

le = LabelEncoder()
df_capped['income'] = le.fit_transform(df_capped['income'])

df_capped["income"].tail(5)

"""**Data Balancing**"""

# Step 1: Separate the features (X) and target (y)
from imblearn.over_sampling import SMOTE
X = df_capped.drop('income', axis=1)
y = df_capped['income']
X = pd.get_dummies(X,drop_first=True)

# Step 2: Apply SMOTE first to balance the classes
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Step 3: Convert the resampled data back to a DataFrame
X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)
y_resampled_df = pd.DataFrame(y_resampled, columns=['income'])

# Step 4: Combine the resampled features and target back into a single DataFrame
df_resampled = pd.concat([X_resampled_df, y_resampled_df], axis=1)

# Step 5: Check the new class distribution to ensure balance
df_resampled['income'].value_counts()

df_resampled

"""Here we had did the Over-Sampling technique to make the data balanced and also we applied the get_dummies function to encode the columns."""

df_resampled["income"].value_counts()

"""Above we rechacked the value counts of income column.

# 7.Feature Selection
"""

selector = SelectKBest(score_func=f_classif, k=15) # Select top 15features
X_new = selector.fit_transform(X_resampled_df, y_resampled_df)

# Get the selected feature's names
selected_feature_indices = selector.get_support(indices=True)
selected_features = X_resampled_df.columns[selected_feature_indices]

print("Selected Features:", selected_features)

# Create a new DataFrame with selected features
X_selected = X_resampled_df[selected_features]

"""We used SelectKBest to select the top 15 features of the resampled dataframe and created a new dataframe, X_selected.

#8. Split Data into Training and Testing Sets
"""

X_train, X_test, y_train, y_test = train_test_split(X_selected, y_resampled_df, test_size=0.2)
y_test=y_test.values.ravel()

"""**Standard Scaling**"""

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform both training and testing data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""We used standard scaler to normalize features by removing the mean and scaling to unit variance.This ensures that all features contribute equally to the model.

# 9. Build the ML Model:
"""

# Create a dictionary of models
models = {
    "Lr": LogisticRegression(),
    "Svm": SVC(),
    "Dt": DecisionTreeClassifier(),
    "Rf": RandomForestClassifier(),
    "Gb": GradientBoostingClassifier()
}

# Train and evaluate each model
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train.values.ravel())
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{name} Accuracy: {accuracy}")

"""Above we trained 5 models an we found that the Gradient boosting model has the high accuracy.

# 10.Model Evaluation
"""

for name, model in models.items():
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    print(f"Model: {name}")
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1 Score: {f1}")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
    xticklabels=["Predicted <50K", "Predicted >50K"],
    yticklabels=["Actual <50K", "Actual >50K"])
    plt.title("Confusion Matrix")
    plt.show()

    # ROC Curve and AUC (only for models that provide probability estimates)
    if hasattr(model, "predict_proba"):
      y_prob = model.predict_proba(X_test)[:, 1]
      fpr, tpr, thresholds = roc_curve(y_test, y_prob)
      roc_auc = auc(fpr, tpr)

      plt.figure()
      plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
      plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
      plt.xlim([0.0, 1.0])
      plt.ylim([0.0, 1.05])
      plt.xlabel('False Positive Rate')
      plt.ylabel('True Positive Rate')
      plt.title(f'ROC Curve for {name}')
      plt.legend(loc="lower right")
      plt.show()
    else:
      print(f"ROC curve not available for {name} (predict_proba not available)")
    print("-" * 30)

"""**Classification Report**"""

for name, model in models.items():
    y_pred = model.predict(X_test)
    print(f"Classification Report for {name}:")
    print(classification_report(y_test, y_pred))
    print("-" * 30)

"""**Evaluation metrices DataFrame**"""

# Data extracted from the classification reports
data = {
    "Model": ["Logistic Regression", "SVM", "Decision Tree", "Random Forest", "Gradient Boosting"],
    "Precision (Class 0)": [0.84, 0.90, 0.84, 0.86, 0.88],
    "Recall (Class 0)": [0.85, 0.79, 0.89, 0.89, 0.88],
    "F1-Score (Class 0)": [0.85, 0.84, 0.86, 0.87, 0.88],
    "Precision (Class 1)": [0.84, 0.81, 0.88, 0.88, 0.87],
    "Recall (Class 1)": [0.84, 0.91, 0.83, 0.85, 0.88],
    "F1-Score (Class 1)": [0.84, 0.86, 0.85, 0.87, 0.88],
    "Accuracy": [0.84, 0.85, 0.86, 0.87, 0.88],
    "Macro Avg F1": [0.84, 0.85, 0.86, 0.87, 0.88],
    "Weighted Avg F1": [0.84, 0.85, 0.86, 0.87, 0.88]
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Optionally, save as CSV
df.to_csv("classification_results.csv", index=False)
df

"""# 11.Hyperparameter Tuning"""

# Define the Gradient Boosting model
gb = GradientBoostingClassifier()

# Define the parameter grid for tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 0.9, 1.0]
}

# Set up the GridSearchCV with cross-validation
grid_search = GridSearchCV(estimator=gb, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

# Fit the model to the training data
grid_search.fit(X_train, y_train)

# Display the best parameters found
print("Best Parameters:", grid_search.best_params_)

"""**Building Best model with the selected parameters**"""

gb = GradientBoostingClassifier(n_estimators=200,learning_rate=0.1,max_depth=5,subsample=1.0,random_state=42)
gb.fit(X_train, y_train.values.ravel())

y_pred = gb.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy*100}%")

"""Once again we train the Gradient Boosting model with the best parameters for decent performance and accuracy.

#12.Save The Model
"""

# Save the fitted model to a file
file= open('My_Model.pkl', 'wb')
pickle.dump(gb,file)

print("Classifier saved successfully.")

"""# 13.Pipeline Making"""

# Identify numerical and categorical features from X_selected
num_features = X_selected.select_dtypes(include=['int64', 'float64']).columns
cat_features = X_selected.select_dtypes(include=['object']).columns

# Create preprocessing steps using selected features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), num_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)],
    remainder='passthrough'  # Handle remaining columns
)

# Define pipeline with the updated preprocessor
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', gb)  # Assuming best_gb is your best model
])

# Fit the pipeline
pipeline.fit(X_train, y_train)

"""**Saving the Pipeline**"""

joblib.dump(pipeline, 'pipeline.joblib')

loaded_pipeline = joblib.load('pipeline.joblib')

"""#14.Test with Unseen Data"""

# Creating an unseen test dataset

unseen_data = pd.DataFrame({
    'age': [37, 50, 29, 41, 55],
    'workclass': ['Private', 'Self-emp-not-inc', 'State-gov', 'Federal-gov', 'Private'],
    'fnlwgt': [215646, 193524, 118372, 154374, 210987],
    'education': ['Bachelors', 'Masters', 'Assoc-acdm', 'HS-grad', 'Doctorate'],
    'education-num': [13, 14, 12, 9, 16],
    'marital-status': ['Married-civ-spouse', 'Divorced', 'Never-married', 'Separated', 'Widowed'],
    'occupation': ['Exec-managerial', 'Sales', 'Prof-specialty', 'Tech-support', 'Craft-repair'],
    'relationship': ['Husband', 'Not-in-family', 'Own-child', 'Unmarried', 'Wife'],
    'race': ['White', 'Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'White'],
    'sex': ['Male', 'Female', 'Male', 'Female', 'Male'],
    'capital-gain': [0, 5000, 0, 2000, 10000],
    'capital-loss': [0, 0, 1000, 500, 0],
    'hours-per-week': [40, 60, 30, 20, 50],
    'native-country': ['United-States', 'India', 'Canada', 'Mexico', 'Germany'],
})

unseen_data

"""Above we created an unseen dataset having 5 rows by giving random values which is not in the training dataset.


"""

y_pred = loaded_pipeline.predict(X_test)
accuracy = accuracy_score(y_test,y_pred)
# Display accuracy on the test set
print("Accuracy on the test set:", accuracy*100)

"""Above we found the testing accuracy of the model we saved and we found that there is no difference with training accuracy.

# 15.Interpretation of Results (Conclusion):

The model trained on the UCI Adult Dataset achieved a training accuracy of 88.7% and a testing accuracy of 88.7%, indicating good performance in predicting income levels. Since there is no difference in accuracy so the model has no underfitting or overfitting.

This means the model performs consistently well on unseen data, making it reliable for real-world predictions.

Overall, the model is well-balanced and effective, with no major signs of underfitting or overfitting.

The UCI Adult dataset, while widely used for classification tasks, has some inherent limitations. It is based on the 1994 U.S. Census, meaning it may not fully represent current socio-economic conditions. Additionally, the dataset may contain biases in gender, race, and income distribution, potentially affecting model fairness. Another limitation is that the dataset lacks certain important features, such as detailed work experience, promotions, or evolving salaries over time, which could improve income prediction. Furthermore, class imbalance exists in the dataset, with a higher proportion of individuals earning ≤50K, which might cause the model to favor the majority class.

In conclusion, The UCI Adult dataset remains a useful benchmark for income prediction, but care must be taken to interpret results fairly, considering its biases. With better feature engineering, regularization, and data balancing, the model could achieve even higher accuracy and better generalization.

#16.Future Work

Exploring deep learning algorithms, such as neural networks, may improve accuracy by capturing complex patterns in the data.

Periodically updating the model with new data can help maintain its relevance and improve performance over time.

Addressing imbalanced data using techniques like oversampling, undersampling, or SMOTE can prevent bias toward the majority class.

Adding more relevant features, such as job tenure, industry type, or economic factors, could enhance the model's predictive power and generalization ability.
"""